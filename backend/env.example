OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_VISION_MODEL=llava:7b
# Lower = faster replies. -1 = no limit.
OLLAMA_NUM_PREDICT=256
# Smaller context = faster (e.g. 2048, 4096). 0 = Ollama default.
OLLAMA_NUM_CTX=2048
FAST_REPLY=true
# Fewer turns = faster inference.
CHAT_MAX_HISTORY_TURNS=4
# Shorter system prompt = faster first token.
CHAT_FAST_PROMPT=true

SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Optional prompt overrides (leave empty to use defaults from prompts.json)
PROMPT_VISION_ANALYZE=
PROMPT_VISION_PROPOSE_TOOL=
